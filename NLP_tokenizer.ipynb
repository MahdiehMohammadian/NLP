{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "l2eVgXWRxXUx"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "# stopwords.words('german')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSYRubO34mYB",
        "outputId": "2e306763-a5f9-4a7a-a296-c941aeb9b283"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define a sample text\n",
        "text = \"The quick brown foxes are jumping over the lazy dogs. cafe or café ? \"\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text)\n",
        "print(doc)\n",
        "\n",
        "# Extract lemmatized tokens\n",
        "lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "print(lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgfBuGmtWGHv",
        "outputId": "ca725a9e-68c8-4978-aa6b-fdbd10cc5cd1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown foxes are jumping over the lazy dogs. cafe or café ? \n",
            "['the', 'quick', 'brown', 'fox', 'be', 'jump', 'over', 'the', 'lazy', 'dog', '.', 'cafe', 'or', 'café', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec = CountVectorizer(strip_accents='unicode', stop_words=stopwords.words('english'))\n",
        "stop_accent_token = vec.fit_transform(lemmatized_tokens)\n",
        "# نمایش ویژگی‌ها و ماتریس ویژگی‌ها\n",
        "print(\"Feature Names:\", vec.get_feature_names_out())\n",
        "print(\"Feature Matrix:\\n\", stop_accent_token.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP2XxYw-gges",
        "outputId": "ac018211-58eb-4011-b631-7477116967ac"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names: ['brown' 'cafe' 'dog' 'fox' 'jump' 'lazy' 'quick']\n",
            "Feature Matrix:\n",
            " [[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1]\n",
            " [1 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0]\n",
            " [0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    }
  ]
}